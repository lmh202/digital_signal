"""
æ§½æ¥”æ¨¡å‹æµ‹è¯•æ•°æ®åˆ†æ - åˆ†ç±»æ¨¡å‹è„šæœ¬
åŠŸèƒ½ï¼š
1. åŠ è½½æå–çš„ç‰¹å¾
2. æ­å»ºç¥ç»ç½‘ç»œåˆ†ç±»æ¨¡å‹
3. è®­ç»ƒæ¨¡å‹å¹¶è®¡ç®—åˆ†ç±»ç²¾åº¦
4. ç»˜åˆ¶æŸå¤±æ›²çº¿
5. åˆ†ææ¾ç´§åº¦ï¼ˆå‹åŠ›å€¼ï¼‰å’Œé¢‘ç‡çš„å˜åŒ–å…³ç³»
"""

import numpy as np
import matplotlib.pyplot as plt
import json
import os
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import warnings
warnings.filterwarnings('ignore')

# å¯¼å…¥ç‰¹å¾æƒé‡é…ç½®
try:
    from feature_weights_config import FEATURE_WEIGHTS, USE_FEATURE_WEIGHTS
    print("âœ… å·²ä» feature_weights_config.py åŠ è½½ç‰¹å¾æƒé‡é…ç½®")
except ImportError:
    print("âš ï¸  æœªæ‰¾åˆ° feature_weights_config.pyï¼Œä½¿ç”¨é»˜è®¤æƒé‡é…ç½®")
    # é»˜è®¤æƒé‡ï¼ˆæ ¹æ®ç‰¹å¾é‡è¦æ€§å›¾è®¾ç½®ï¼‰
    FEATURE_WEIGHTS = np.array([
        0.02, 0.05, 0.06, 0.08, 0.08, 0.11, 0.05, 0.05,
        0.03, 0.12, 0.16, 0.04, 0.03, 0.09, 0.05, 0.03
    ])
    USE_FEATURE_WEIGHTS = True

# å°è¯•å¯¼å…¥æ·±åº¦å­¦ä¹ æ¡†æ¶
try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    from torch.utils.data import DataLoader, TensorDataset
    USE_TORCH = True
    print("ä½¿ç”¨ PyTorch ä½œä¸ºæ·±åº¦å­¦ä¹ æ¡†æ¶")
except ImportError:
    USE_TORCH = False
    from sklearn.neural_network import MLPClassifier
    print("PyTorch æœªå®‰è£…ï¼Œä½¿ç”¨ sklearn MLPClassifier")

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# å‚æ•°è®¾ç½®
FEATURE_DIR = "features"
MODEL_DIR = "models"
RESULT_DIR = "results"

# è®­ç»ƒå‚æ•°ï¼ˆä¼˜åŒ–ç‰ˆ - åŸºäº87.65%æœ€ä½³é…ç½®å¾®è°ƒï¼‰
EPOCHS = 1200  # å¢åŠ è®­ç»ƒè½®æ¬¡ä»¥æ‰¾åˆ°æ›´ä¼˜è§£
BATCH_SIZE = 20  # ä¿æŒä¸­ç­‰æ‰¹é‡
LEARNING_RATE = 0.0017  # ç¨å¾®é™ä½å­¦ä¹ ç‡ï¼Œæ›´å¹³ç¨³è®­ç»ƒ
HIDDEN_SIZES = [320, 192, 96, 48]  # æ¢å¤87.65%æ—¶çš„æ¶æ„
DROPOUT_RATE = 0.2  # é€‚ä¸­çš„dropout
TEST_SIZE = 0.2
RANDOM_STATE = 42

# æ—©åœå‚æ•°
EARLY_STOPPING_PATIENCE = 1200  # æ›´å¤§çš„è€å¿ƒ
MIN_DELTA = 0.0002  # æ›´å°çš„æ”¹è¿›é˜ˆå€¼


def create_output_dirs():
    """åˆ›å»ºè¾“å‡ºç›®å½•"""
    for d in [MODEL_DIR, RESULT_DIR]:
        os.makedirs(d, exist_ok=True)
    print(f"å·²åˆ›å»ºè¾“å‡ºç›®å½•: {MODEL_DIR}, {RESULT_DIR}")


def load_features():
    """åŠ è½½ç‰¹å¾å’Œæ ‡ç­¾"""
    print("\nåŠ è½½ç‰¹å¾æ•°æ®...")
    
    X = np.load(os.path.join(FEATURE_DIR, "features.npy"))
    y = np.load(os.path.join(FEATURE_DIR, "labels.npy"))
    
    with open(os.path.join(FEATURE_DIR, "feature_names.json"), 'r', encoding='utf-8') as f:
        feature_names = json.load(f)
    
    print(f"ç‰¹å¾çŸ©é˜µå½¢çŠ¶: {X.shape}")
    print(f"æ ‡ç­¾æ•°é‡: {len(y)}")
    print(f"ç±»åˆ«: {np.unique(y)}")
    
    return X, y, feature_names


def print_feature_weights(feature_names):
    """æ‰“å°å½“å‰çš„ç‰¹å¾æƒé‡é…ç½®"""
    print("\n" + "="*60)
    print("ç‰¹å¾æƒé‡é…ç½®")
    print("="*60)
    print(f"å¯ç”¨çŠ¶æ€: {'âœ… å·²å¯ç”¨' if USE_FEATURE_WEIGHTS else 'âŒ æœªå¯ç”¨'}")
    
    if USE_FEATURE_WEIGHTS:
        print(f"\n{'ç´¢å¼•':<6} {'ç‰¹å¾å':<30} {'æƒé‡':<10} {'å½’ä¸€åŒ–æƒé‡':<12}")
        print("-" * 60)
        
        # å½’ä¸€åŒ–æƒé‡ï¼ˆæ€»å’Œ=ç‰¹å¾æ•°ï¼‰
        weights_normalized = FEATURE_WEIGHTS / FEATURE_WEIGHTS.sum() * len(feature_names)
        
        for i, (name, weight, norm_weight) in enumerate(zip(feature_names, FEATURE_WEIGHTS, weights_normalized)):
            print(f"{i:<6} {name:<30} {weight:<10.4f} {norm_weight:<12.4f}")
        
        print("-" * 60)
        print(f"æƒé‡æ€»å’Œ: {FEATURE_WEIGHTS.sum():.4f}")
        print(f"å½’ä¸€åŒ–æƒé‡æ€»å’Œ: {weights_normalized.sum():.4f}")
        print(f"å»ºè®®: æ ¹æ®ç‰¹å¾é‡è¦æ€§å›¾è°ƒæ•´FEATURE_WEIGHTSæ•°ç»„")
    
    print("="*60)


def preprocess_data(X, y, test_size=TEST_SIZE):
    """æ•°æ®é¢„å¤„ç†"""
    print("\næ•°æ®é¢„å¤„ç†...")
    
    # æ ‡ç­¾ç¼–ç 
    label_encoder = LabelEncoder()
    y_encoded = label_encoder.fit_transform(y)
    
    # åº”ç”¨ç‰¹å¾æƒé‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if USE_FEATURE_WEIGHTS:
        print(f"åº”ç”¨ç‰¹å¾æƒé‡...")
        # ç¡®ä¿æƒé‡ç»´åº¦åŒ¹é…
        if len(FEATURE_WEIGHTS) != X.shape[1]:
            print(f"âš ï¸  è­¦å‘Šï¼šç‰¹å¾æƒé‡ç»´åº¦({len(FEATURE_WEIGHTS)})ä¸ç‰¹å¾æ•°({X.shape[1]})ä¸åŒ¹é…ï¼Œå°†ä¸åº”ç”¨æƒé‡")
            X_weighted = X.copy()
        else:
            # å½’ä¸€åŒ–æƒé‡ï¼Œä½¿å…¶å’Œä¸ºç‰¹å¾æ•°ï¼ˆä¿æŒæ€»ä½“å°ºåº¦ï¼‰
            weights_normalized = FEATURE_WEIGHTS / FEATURE_WEIGHTS.sum() * X.shape[1]
            X_weighted = X * weights_normalized[np.newaxis, :]
            print(f"âœ… å·²åº”ç”¨ç‰¹å¾æƒé‡ï¼Œå½’ä¸€åŒ–å› å­={weights_normalized.sum()/X.shape[1]:.4f}")
            print(f"   æƒé‡èŒƒå›´: [{weights_normalized.min():.4f}, {weights_normalized.max():.4f}]")
    else:
        print("æœªå¯ç”¨ç‰¹å¾æƒé‡ï¼Œä½¿ç”¨åŸå§‹ç‰¹å¾")
        X_weighted = X.copy()
    
    # æ•°æ®æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X_weighted)
    
    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y_encoded, test_size=test_size, random_state=RANDOM_STATE, stratify=y_encoded
    )
    
    print(f"è®­ç»ƒé›†å¤§å°: {len(X_train)}")
    print(f"æµ‹è¯•é›†å¤§å°: {len(X_test)}")
    print(f"ç±»åˆ«æ˜ å°„: {dict(zip(label_encoder.classes_, range(len(label_encoder.classes_))))}")
    
    return X_train, X_test, y_train, y_test, scaler, label_encoder


class NeuralNetwork(nn.Module):
    """ä¼˜åŒ–çš„PyTorchç¥ç»ç½‘ç»œæ¨¡å‹ - ç›®æ ‡90%+å‡†ç¡®ç‡"""
    def __init__(self, input_size, hidden_sizes, num_classes, dropout_rate=DROPOUT_RATE):
        super(NeuralNetwork, self).__init__()
        
        self.input_layer = nn.Sequential(
            nn.Linear(input_size, hidden_sizes[0]),
            nn.BatchNorm1d(hidden_sizes[0]),
            nn.ReLU(),
            nn.Dropout(dropout_rate)
        )
        
        # æ„å»ºéšè—å±‚ï¼ˆå¸¦æ®‹å·®è¿æ¥ï¼‰
        self.hidden_layers = nn.ModuleList()
        for i in range(len(hidden_sizes) - 1):
            layer = nn.Sequential(
                nn.Linear(hidden_sizes[i], hidden_sizes[i + 1]),
                nn.BatchNorm1d(hidden_sizes[i + 1]),
                nn.ReLU(),
                nn.Dropout(dropout_rate * 0.8)  # åç»­å±‚dropoutç¨å°
            )
            self.hidden_layers.append(layer)
        
        # è¾“å‡ºå±‚
        self.output_layer = nn.Linear(hidden_sizes[-1], num_classes)
        
        # æ®‹å·®è¿æ¥çš„æŠ•å½±å±‚ï¼ˆå¦‚æœç»´åº¦ä¸åŒ¹é…ï¼‰
        self.residual_projections = nn.ModuleList()
        for i in range(len(hidden_sizes) - 1):
            if hidden_sizes[i] != hidden_sizes[i + 1]:
                self.residual_projections.append(
                    nn.Linear(hidden_sizes[i], hidden_sizes[i + 1])
                )
            else:
                self.residual_projections.append(None)
    
    def forward(self, x):
        x = self.input_layer(x)
        
        # é€šè¿‡éšè—å±‚å¹¶åº”ç”¨æ®‹å·®è¿æ¥
        for i, layer in enumerate(self.hidden_layers):
            identity = x
            x = layer(x)
            
            # æ®‹å·®è¿æ¥
            if self.residual_projections[i] is not None:
                identity = self.residual_projections[i](identity)
            x = x + identity * 0.3  # åŠ æƒæ®‹å·®è¿æ¥
        
        x = self.output_layer(x)
        return x


def train_pytorch_model(X_train, X_test, y_train, y_test, num_classes):
    """ä½¿ç”¨ PyTorch è®­ç»ƒä¼˜åŒ–çš„ç¥ç»ç½‘ç»œæ¨¡å‹"""
    print("\nä½¿ç”¨ PyTorch è®­ç»ƒä¼˜åŒ–çš„ç¥ç»ç½‘ç»œ...")
    print(f"ç›®æ ‡: è¾¾åˆ°90%+å‡†ç¡®ç‡")
    
    # è½¬æ¢ä¸ºå¼ é‡
    X_train_tensor = torch.FloatTensor(X_train)
    y_train_tensor = torch.LongTensor(y_train)
    X_test_tensor = torch.FloatTensor(X_test)
    y_test_tensor = torch.LongTensor(y_test)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)
    
    # åˆ›å»ºæ¨¡å‹
    input_size = X_train.shape[1]
    model = NeuralNetwork(input_size, HIDDEN_SIZES, num_classes)
    print(f"æ¨¡å‹ç»“æ„:\n{model}")
    print(f"\næ¨¡å‹å‚æ•°ç»Ÿè®¡:")
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"  æ€»å‚æ•°é‡: {total_params:,}")
    print(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    
    # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)  # æ ‡ç­¾å¹³æ»‘
    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=5e-4)  # AdamWä¼˜åŒ–å™¨
    
    # ä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆæ›´å¹³æ»‘ï¼‰
    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
        optimizer, T_0=50, T_mult=2, eta_min=1e-6
    )
    
    # è®­ç»ƒå†å²
    train_losses = []
    test_losses = []
    train_accuracies = []
    test_accuracies = []
    
    # æœ€ä½³æ¨¡å‹è¿½è¸ª
    best_test_acc = 0.0
    best_epoch = 0
    best_model_state = None
    epochs_without_improvement = 0
    
    print(f"\nå¼€å§‹è®­ç»ƒ (æ€»å…± {EPOCHS} è½®)...")
    print(f"æ—©åœç­–ç•¥: {EARLY_STOPPING_PATIENCE} è½®æ— æ”¹è¿›åˆ™åœæ­¢")
    print("="*80)
    
    # è®­ç»ƒå¾ªç¯
    for epoch in range(EPOCHS):
        model.train()
        epoch_loss = 0
        correct = 0
        total = 0
        
        for batch_X, batch_y in train_loader:
            optimizer.zero_grad()
            outputs = model(batch_X)
            loss = criterion(outputs, batch_y)
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            
            epoch_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            total += batch_y.size(0)
            correct += (predicted == batch_y).sum().item()
        
        scheduler.step()
        
        # è®¡ç®—è®­ç»ƒæŒ‡æ ‡
        train_loss = epoch_loss / len(train_loader)
        train_acc = correct / total
        train_losses.append(train_loss)
        train_accuracies.append(train_acc)
        
        # è®¡ç®—æµ‹è¯•æŒ‡æ ‡
        model.eval()
        with torch.no_grad():
            test_outputs = model(X_test_tensor)
            test_loss = criterion(test_outputs, y_test_tensor).item()
            _, test_predicted = torch.max(test_outputs, 1)
            test_acc = (test_predicted == y_test_tensor).sum().item() / len(y_test_tensor)
        
        test_losses.append(test_loss)
        test_accuracies.append(test_acc)
        
        # æ›´æ–°æœ€ä½³æ¨¡å‹
        if test_acc > best_test_acc + MIN_DELTA:
            best_test_acc = test_acc
            best_epoch = epoch + 1
            best_model_state = model.state_dict().copy()
            epochs_without_improvement = 0
            
            # å®æ—¶ä¿å­˜æœ€ä½³æ¨¡å‹
            torch.save({
                'epoch': best_epoch,
                'model_state_dict': best_model_state,
                'best_test_acc': best_test_acc,
                'optimizer_state_dict': optimizer.state_dict(),
            }, os.path.join(MODEL_DIR, "best_model.pth"))
            
            status = "ğŸŒŸ æ–°æœ€ä½³!"
        else:
            epochs_without_improvement += 1
            status = ""
        
        # å®šæœŸæ‰“å°è¿›åº¦
        if (epoch + 1) % 20 == 0 or status:
            current_lr = optimizer.param_groups[0]['lr']
            print(f"Epoch [{epoch+1:4d}/{EPOCHS}] - "
                  f"LR: {current_lr:.2e} - "
                  f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} - "
                  f"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f} {status}")
        
        # æ—©åœæ£€æŸ¥
        if epochs_without_improvement >= EARLY_STOPPING_PATIENCE:
            print(f"\nâš ï¸  æ—©åœè§¦å‘! {EARLY_STOPPING_PATIENCE} è½®æ— æ”¹è¿›")
            print(f"åœ¨ç¬¬ {best_epoch} è½®è¾¾åˆ°æœ€ä½³å‡†ç¡®ç‡: {best_test_acc:.4f} ({best_test_acc*100:.2f}%)")
            break
    
    # åŠ è½½æœ€ä½³æ¨¡å‹
    if best_model_state is not None:
        model.load_state_dict(best_model_state)
        print(f"\nâœ… å·²åŠ è½½æœ€ä½³æ¨¡å‹ (Epoch {best_epoch}, å‡†ç¡®ç‡: {best_test_acc*100:.2f}%)")
    
    # ä½¿ç”¨æœ€ä½³æ¨¡å‹è¿›è¡Œæœ€ç»ˆè¯„ä¼°
    model.eval()
    with torch.no_grad():
        final_outputs = model(X_test_tensor)
        _, final_predictions = torch.max(final_outputs, 1)
        final_predictions = final_predictions.numpy()
    
    history = {
        'train_loss': train_losses,
        'test_loss': test_losses,
        'train_accuracy': train_accuracies,
        'test_accuracy': test_accuracies,
        'best_epoch': best_epoch,
        'best_test_acc': best_test_acc
    }
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹ä¿¡æ¯
    torch.save({
        'model_state_dict': model.state_dict(),
        'hidden_sizes': HIDDEN_SIZES,
        'num_classes': num_classes,
        'best_test_acc': best_test_acc,
        'best_epoch': best_epoch,
        'history': history
    }, os.path.join(MODEL_DIR, "final_best_model.pth"))
    
    print(f"\nğŸ“Š è®­ç»ƒå®Œæˆç»Ÿè®¡:")
    print(f"  æœ€ä½³å‡†ç¡®ç‡: {best_test_acc*100:.2f}% (ç¬¬ {best_epoch} è½®)")
    print(f"  æœ€ç»ˆè®­ç»ƒå‡†ç¡®ç‡: {train_accuracies[-1]*100:.2f}%")
    print(f"  å®é™…è®­ç»ƒè½®æ•°: {len(train_losses)}")
    
    return model, final_predictions, history


def train_sklearn_model(X_train, X_test, y_train, y_test, num_classes):
    """ä½¿ç”¨ sklearn è®­ç»ƒæ¨¡å‹"""
    print("\nä½¿ç”¨ sklearn MLPClassifier è®­ç»ƒç¥ç»ç½‘ç»œ...")
    
    model = MLPClassifier(
        hidden_layer_sizes=tuple(HIDDEN_SIZES),
        activation='relu',
        solver='adam',
        alpha=1e-4,
        batch_size=BATCH_SIZE,
        learning_rate_init=LEARNING_RATE,
        max_iter=EPOCHS,
        random_state=RANDOM_STATE,
        early_stopping=False,
        verbose=True
    )
    
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)
    
    # sklearn çš„ loss_curve_ åªæœ‰è®­ç»ƒæŸå¤±
    history = {
        'train_loss': model.loss_curve_,
        'test_loss': [],  # sklearn ä¸æä¾›æµ‹è¯•æŸå¤±
        'train_accuracy': [],
        'test_accuracy': []
    }
    
    return model, predictions, history


def plot_loss_curves(history, save_path):
    """ç»˜åˆ¶æŸå¤±æ›²çº¿ï¼ˆæ ‡æ³¨æœ€ä½³æ¨¡å‹ä½ç½®ï¼‰"""
    fig, axes = plt.subplots(1, 2, figsize=(16, 5))
    
    # è·å–æœ€ä½³epochä½ç½®
    best_epoch = history.get('best_epoch', 0)
    best_test_acc = history.get('best_test_acc', 0)
    
    # æŸå¤±æ›²çº¿
    axes[0].plot(history['train_loss'], 'b-', label='è®­ç»ƒæŸå¤±', linewidth=2, alpha=0.7)
    if len(history['test_loss']) > 0:
        axes[0].plot(history['test_loss'], 'r-', label='æµ‹è¯•æŸå¤±', linewidth=2, alpha=0.7)
        # æ ‡æ³¨æœ€ä½³æ¨¡å‹ä½ç½®
        if best_epoch > 0:
            axes[0].axvline(x=best_epoch-1, color='g', linestyle='--', linewidth=2, 
                           label=f'æœ€ä½³æ¨¡å‹ (Epoch {best_epoch})')
            axes[0].scatter([best_epoch-1], [history['test_loss'][best_epoch-1]], 
                           color='g', s=100, zorder=5, marker='*')
    axes[0].set_xlabel('Epoch')
    axes[0].set_ylabel('æŸå¤±')
    axes[0].set_title('è®­ç»ƒå’Œæµ‹è¯•æŸå¤±æ›²çº¿')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)
    
    # å‡†ç¡®ç‡æ›²çº¿
    if len(history['train_accuracy']) > 0:
        axes[1].plot(history['train_accuracy'], 'b-', label='è®­ç»ƒå‡†ç¡®ç‡', linewidth=2, alpha=0.7)
        axes[1].plot(history['test_accuracy'], 'r-', label='æµ‹è¯•å‡†ç¡®ç‡', linewidth=2, alpha=0.7)
        
        # æ ‡æ³¨æœ€ä½³æ¨¡å‹ä½ç½®
        if best_epoch > 0:
            axes[1].axvline(x=best_epoch-1, color='g', linestyle='--', linewidth=2, 
                           label=f'æœ€ä½³: {best_test_acc*100:.2f}%')
            axes[1].scatter([best_epoch-1], [history['test_accuracy'][best_epoch-1]], 
                           color='g', s=100, zorder=5, marker='*')
            
            # æ·»åŠ æ–‡æœ¬æ ‡æ³¨
            axes[1].text(best_epoch-1, history['test_accuracy'][best_epoch-1] + 0.02,
                        f'{best_test_acc*100:.2f}%', ha='center', fontsize=10,
                        bbox=dict(boxstyle='round,pad=0.5', facecolor='yellow', alpha=0.7))
        
        # æ·»åŠ 90%å‡†ç¡®ç‡åŸºå‡†çº¿
        axes[1].axhline(y=0.90, color='orange', linestyle=':', linewidth=2, 
                       label='ç›®æ ‡: 90%', alpha=0.6)
        
        axes[1].set_xlabel('Epoch')
        axes[1].set_ylabel('å‡†ç¡®ç‡')
        axes[1].set_title('è®­ç»ƒå’Œæµ‹è¯•å‡†ç¡®ç‡æ›²çº¿')
        axes[1].set_ylim([0.5, 1.0])  # å›ºå®šyè½´èŒƒå›´ä»¥ä¾¿è§‚å¯Ÿ
        axes[1].legend()
        axes[1].grid(True, alpha=0.3)
    else:
        axes[1].text(0.5, 0.5, 'å‡†ç¡®ç‡æ•°æ®ä¸å¯ç”¨', ha='center', va='center', fontsize=14)
        axes[1].set_title('å‡†ç¡®ç‡æ›²çº¿')
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    plt.close()
    print(f"æŸå¤±æ›²çº¿å·²ä¿å­˜: {save_path}")


def plot_confusion_matrix(y_true, y_pred, labels, save_path):
    """ç»˜åˆ¶æ··æ·†çŸ©é˜µ"""
    cm = confusion_matrix(y_true, y_pred)
    
    fig, ax = plt.subplots(figsize=(10, 8))
    im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
    ax.figure.colorbar(im, ax=ax)
    
    # è®¾ç½®åˆ»åº¦
    ax.set(xticks=np.arange(cm.shape[1]),
           yticks=np.arange(cm.shape[0]),
           xticklabels=labels,
           yticklabels=labels,
           title='æ··æ·†çŸ©é˜µ',
           ylabel='çœŸå®æ ‡ç­¾',
           xlabel='é¢„æµ‹æ ‡ç­¾')
    
    # æ—‹è½¬xè½´æ ‡ç­¾
    plt.setp(ax.get_xticklabels(), rotation=45, ha="right", rotation_mode="anchor")
    
    # åœ¨æ ¼å­ä¸­æ·»åŠ æ•°å€¼
    thresh = cm.max() / 2.
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            ax.text(j, i, format(cm[i, j], 'd'),
                    ha="center", va="center",
                    color="white" if cm[i, j] > thresh else "black")
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    plt.close()
    print(f"æ··æ·†çŸ©é˜µå·²ä¿å­˜: {save_path}")


def analyze_feature_importance(X, y, feature_names, save_path):
    """åˆ†æç‰¹å¾é‡è¦æ€§"""
    from sklearn.ensemble import RandomForestClassifier
    
    print("\nåˆ†æç‰¹å¾é‡è¦æ€§...")
    
    rf = RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE)
    rf.fit(X, y)
    
    importances = rf.feature_importances_
    indices = np.argsort(importances)[::-1]
    
    # ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§å›¾
    fig, ax = plt.subplots(figsize=(12, 6))
    
    colors = plt.cm.viridis(np.linspace(0, 1, len(feature_names)))
    bars = ax.bar(range(len(feature_names)), importances[indices], color=colors)
    
    ax.set_xlabel('ç‰¹å¾')
    ax.set_ylabel('é‡è¦æ€§')
    ax.set_title('ç‰¹å¾é‡è¦æ€§åˆ†æ')
    ax.set_xticks(range(len(feature_names)))
    ax.set_xticklabels([feature_names[i] for i in indices], rotation=45, ha='right')
    ax.grid(True, alpha=0.3, axis='y')
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    plt.close()
    print(f"ç‰¹å¾é‡è¦æ€§å›¾å·²ä¿å­˜: {save_path}")
    
    print("\nç‰¹å¾é‡è¦æ€§æ’å:")
    for i, idx in enumerate(indices[:10]):
        print(f"  {i+1}. {feature_names[idx]}: {importances[idx]:.4f}")


def analyze_tightness_frequency_detailed():
    """è¯¦ç»†åˆ†ææ¾ç´§åº¦ä¸é¢‘ç‡çš„å…³ç³»"""
    print("\n" + "="*60)
    print("æ¾ç´§åº¦ä¸é¢‘ç‡å…³ç³»è¯¦ç»†åˆ†æ")
    print("="*60)
    
    # åŠ è½½å…³ç³»æ•°æ®
    with open(os.path.join(FEATURE_DIR, "tightness_frequency_relationship.json"), 'r', encoding='utf-8') as f:
        relationship = json.load(f)
    
    tightness = relationship['tightness']
    main_frequencies = relationship['main_frequencies']
    spectral_centroids = relationship['spectral_centroids']
    
    # è®¡ç®—ç›¸å…³ç³»æ•°
    from scipy import stats
    
    corr_main, p_main = stats.pearsonr(tightness, main_frequencies)
    corr_centroid, p_centroid = stats.pearsonr(tightness, spectral_centroids)
    
    print(f"\næ¾ç´§åº¦ä¸ä¸»é¢‘ç‡çš„ç›¸å…³æ€§:")
    print(f"  Pearsonç›¸å…³ç³»æ•°: {corr_main:.4f}")
    print(f"  på€¼: {p_main:.6f}")
    print(f"  ç»“è®º: {'æ˜¾è‘—ç›¸å…³' if p_main < 0.05 else 'ä¸æ˜¾è‘—ç›¸å…³'}")
    
    print(f"\næ¾ç´§åº¦ä¸é¢‘è°±è´¨å¿ƒçš„ç›¸å…³æ€§:")
    print(f"  Pearsonç›¸å…³ç³»æ•°: {corr_centroid:.4f}")
    print(f"  på€¼: {p_centroid:.6f}")
    print(f"  ç»“è®º: {'æ˜¾è‘—ç›¸å…³' if p_centroid < 0.05 else 'ä¸æ˜¾è‘—ç›¸å…³'}")
    
    # çº¿æ€§å›å½’åˆ†æ
    slope_main, intercept_main, r_main, _, _ = stats.linregress(tightness, main_frequencies)
    slope_centroid, intercept_centroid, r_centroid, _, _ = stats.linregress(tightness, spectral_centroids)
    
    # ç»˜åˆ¶è¯¦ç»†å…³ç³»å›¾
    fig, axes = plt.subplots(2, 2, figsize=(14, 12))
    
    # ä¸»é¢‘ç‡æ•£ç‚¹å›¾å’Œå›å½’çº¿
    axes[0, 0].scatter(tightness, main_frequencies, s=100, c='blue', alpha=0.7, edgecolors='black')
    x_line = np.array([min(tightness), max(tightness)])
    axes[0, 0].plot(x_line, slope_main * x_line + intercept_main, 'r--', linewidth=2, 
                    label=f'çº¿æ€§å›å½’: y = {slope_main:.4f}x + {intercept_main:.2f}')
    axes[0, 0].set_xlabel('æ¾ç´§åº¦ï¼ˆå‹åŠ›å€¼ï¼‰')
    axes[0, 0].set_ylabel('å¹³å‡ä¸»é¢‘ç‡ (Hz)')
    axes[0, 0].set_title(f'æ¾ç´§åº¦ä¸ä¸»é¢‘ç‡å…³ç³» (RÂ² = {r_main**2:.4f})')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)
    
    # é¢‘è°±è´¨å¿ƒæ•£ç‚¹å›¾å’Œå›å½’çº¿
    axes[0, 1].scatter(tightness, spectral_centroids, s=100, c='red', alpha=0.7, edgecolors='black')
    axes[0, 1].plot(x_line, slope_centroid * x_line + intercept_centroid, 'b--', linewidth=2,
                    label=f'çº¿æ€§å›å½’: y = {slope_centroid:.4f}x + {intercept_centroid:.2f}')
    axes[0, 1].set_xlabel('æ¾ç´§åº¦ï¼ˆå‹åŠ›å€¼ï¼‰')
    axes[0, 1].set_ylabel('å¹³å‡é¢‘è°±è´¨å¿ƒ (Hz)')
    axes[0, 1].set_title(f'æ¾ç´§åº¦ä¸é¢‘è°±è´¨å¿ƒå…³ç³» (RÂ² = {r_centroid**2:.4f})')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)
    
    # ä¸»é¢‘ç‡å˜åŒ–è¶‹åŠ¿
    axes[1, 0].plot(tightness, main_frequencies, 'bo-', markersize=10, linewidth=2)
    for i, (t, f) in enumerate(zip(tightness, main_frequencies)):
        axes[1, 0].annotate(f'{f:.0f}Hz', (t, f), textcoords="offset points", 
                           xytext=(0, 10), ha='center', fontsize=9)
    axes[1, 0].set_xlabel('æ¾ç´§åº¦ï¼ˆå‹åŠ›å€¼ï¼‰')
    axes[1, 0].set_ylabel('å¹³å‡ä¸»é¢‘ç‡ (Hz)')
    axes[1, 0].set_title('ä¸»é¢‘ç‡éšæ¾ç´§åº¦çš„å˜åŒ–è¶‹åŠ¿')
    axes[1, 0].grid(True, alpha=0.3)
    
    # é¢‘è°±è´¨å¿ƒå˜åŒ–è¶‹åŠ¿
    axes[1, 1].plot(tightness, spectral_centroids, 'ro-', markersize=10, linewidth=2)
    for i, (t, c) in enumerate(zip(tightness, spectral_centroids)):
        axes[1, 1].annotate(f'{c:.0f}Hz', (t, c), textcoords="offset points",
                           xytext=(0, 10), ha='center', fontsize=9)
    axes[1, 1].set_xlabel('æ¾ç´§åº¦ï¼ˆå‹åŠ›å€¼ï¼‰')
    axes[1, 1].set_ylabel('å¹³å‡é¢‘è°±è´¨å¿ƒ (Hz)')
    axes[1, 1].set_title('é¢‘è°±è´¨å¿ƒéšæ¾ç´§åº¦çš„å˜åŒ–è¶‹åŠ¿')
    axes[1, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    save_path = os.path.join(RESULT_DIR, "tightness_frequency_detailed.png")
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    plt.close()
    print(f"\nè¯¦ç»†å…³ç³»å›¾å·²ä¿å­˜: {save_path}")
    
    # ä¿å­˜åˆ†æç»“æœ
    analysis_results = {
        'main_frequency': {
            'correlation': corr_main,
            'p_value': p_main,
            'slope': slope_main,
            'intercept': intercept_main,
            'r_squared': r_main**2
        },
        'spectral_centroid': {
            'correlation': corr_centroid,
            'p_value': p_centroid,
            'slope': slope_centroid,
            'intercept': intercept_centroid,
            'r_squared': r_centroid**2
        }
    }
    
    with open(os.path.join(RESULT_DIR, "frequency_analysis.json"), 'w', encoding='utf-8') as f:
        json.dump(analysis_results, f, ensure_ascii=False, indent=2)
    
    return analysis_results


def main():
    """ä¸»å‡½æ•°"""
    print("="*60)
    print("æ§½æ¥”æ¨¡å‹æµ‹è¯•æ•°æ®åˆ†æ - åˆ†ç±»æ¨¡å‹")
    print("="*60)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    create_output_dirs()
    
    # åŠ è½½ç‰¹å¾
    X, y, feature_names = load_features()
    
    # æ‰“å°ç‰¹å¾æƒé‡é…ç½®
    print_feature_weights(feature_names)
    
    # æ•°æ®é¢„å¤„ç†
    X_train, X_test, y_train, y_test, scaler, label_encoder = preprocess_data(X, y)
    num_classes = len(label_encoder.classes_)
    
    # è®­ç»ƒæ¨¡å‹
    if USE_TORCH:
        model, predictions, history = train_pytorch_model(X_train, X_test, y_train, y_test, num_classes)
    else:
        model, predictions, history = train_sklearn_model(X_train, X_test, y_train, y_test, num_classes)
    
    # è®¡ç®—åˆ†ç±»ç²¾åº¦
    accuracy = accuracy_score(y_test, predictions)
    
    # è·å–æœ€ä½³å‡†ç¡®ç‡ï¼ˆå¦‚æœæœ‰ï¼‰
    best_accuracy = history.get('best_test_acc', accuracy)
    best_epoch = history.get('best_epoch', 0)
    
    print(f"\n{'='*80}")
    print(f"ğŸ¯ æœ€ç»ˆåˆ†ç±»ç»“æœ")
    print(f"{'='*80}")
    if best_epoch > 0:
        print(f"âœ¨ æœ€ä½³æµ‹è¯•å‡†ç¡®ç‡: {best_accuracy:.4f} ({best_accuracy*100:.2f}%) - ç¬¬ {best_epoch} è½®")
        print(f"   å½“å‰æ¨¡å‹å‡†ç¡®ç‡: {accuracy:.4f} ({accuracy*100:.2f}%)")
        
        if best_accuracy >= 0.90:
            print(f"ğŸ‰ æ­å–œ! å·²è¾¾åˆ°90%+å‡†ç¡®ç‡ç›®æ ‡!")
        else:
            print(f"ğŸ“ˆ è·ç¦»90%ç›®æ ‡è¿˜å·®: {(0.90 - best_accuracy)*100:.2f}%")
    else:
        print(f"æµ‹è¯•é›†å‡†ç¡®ç‡: {accuracy:.4f} ({accuracy*100:.2f}%)")
    print(f"{'='*80}")
    
    # è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
    print("\nåˆ†ç±»æŠ¥å‘Š:")
    class_labels = [str(c) for c in label_encoder.classes_]
    print(classification_report(y_test, predictions, target_names=class_labels))
    
    # ç»˜åˆ¶æŸå¤±æ›²çº¿
    plot_loss_curves(history, os.path.join(RESULT_DIR, "loss_curves.png"))
    
    # ç»˜åˆ¶æ··æ·†çŸ©é˜µ
    plot_confusion_matrix(y_test, predictions, class_labels, 
                         os.path.join(RESULT_DIR, "confusion_matrix.png"))
    
    # åˆ†æç‰¹å¾é‡è¦æ€§
    analyze_feature_importance(X, y, feature_names, 
                              os.path.join(RESULT_DIR, "feature_importance.png"))
    
    # è¯¦ç»†åˆ†ææ¾ç´§åº¦ä¸é¢‘ç‡å…³ç³»
    analyze_tightness_frequency_detailed()
    
    # ä¿å­˜åˆ†ç±»ç»“æœ
    results = {
        'accuracy': float(accuracy),
        'best_accuracy': float(best_accuracy) if best_epoch > 0 else float(accuracy),
        'best_epoch': int(best_epoch) if best_epoch > 0 else None,
        'num_classes': num_classes,
        'class_labels': class_labels,
        'train_samples': len(y_train),
        'test_samples': len(y_test),
        'feature_dim': X.shape[1],
        'model_architecture': {
            'hidden_sizes': HIDDEN_SIZES,
            'dropout_rate': DROPOUT_RATE,
            'epochs': EPOCHS,
            'actual_epochs': len(history.get('train_loss', [])),
            'batch_size': BATCH_SIZE,
            'learning_rate': LEARNING_RATE,
            'early_stopping_patience': EARLY_STOPPING_PATIENCE
        },
        'achieved_90_percent': best_accuracy >= 0.90 if best_epoch > 0 else accuracy >= 0.90
    }
    
    with open(os.path.join(RESULT_DIR, "classification_results.json"), 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"\n{'='*80}")
    print("âœ… åˆ†æå®Œæˆ")
    print(f"{'='*80}")
    print(f"ç»“æœå·²ä¿å­˜åˆ°: {RESULT_DIR}/")
    print("  ğŸ“Š loss_curves.png: æŸå¤±å’Œå‡†ç¡®ç‡æ›²çº¿ï¼ˆæ ‡æ³¨æœ€ä½³æ¨¡å‹ï¼‰")
    print("  ğŸ“ˆ confusion_matrix.png: æ··æ·†çŸ©é˜µ")
    print("  ğŸ” feature_importance.png: ç‰¹å¾é‡è¦æ€§")
    print("  ğŸ“‰ tightness_frequency_detailed.png: æ¾ç´§åº¦ä¸é¢‘ç‡å…³ç³»")
    print("  ğŸ“„ classification_results.json: åˆ†ç±»ç»“æœæ±‡æ€»")
    print("  ğŸ“„ frequency_analysis.json: é¢‘ç‡åˆ†æç»“æœ")
    print(f"\næ¨¡å‹å·²ä¿å­˜åˆ°: {MODEL_DIR}/")
    print("  ğŸŒŸ best_model.pth: æœ€ä½³æ¨¡å‹æ£€æŸ¥ç‚¹")
    print("  ğŸ’¾ final_best_model.pth: æœ€ç»ˆæœ€ä½³æ¨¡å‹ï¼ˆåŒ…å«å®Œæ•´ä¿¡æ¯ï¼‰")
    
    if best_accuracy >= 0.90:
        print(f"\nğŸ‰ğŸ‰ğŸ‰ æˆåŠŸè¾¾æˆç›®æ ‡! æœ€ä½³å‡†ç¡®ç‡: {best_accuracy*100:.2f}% ğŸ‰ğŸ‰ğŸ‰")
    else:
        print(f"\nğŸ’¡ æç¤º: å½“å‰æœ€ä½³å‡†ç¡®ç‡ {best_accuracy*100:.2f}%")
        print(f"   å¯ä»¥å°è¯•:")
        print(f"   1. è°ƒæ•´ feature_weights_config.py ä¸­çš„ç‰¹å¾æƒé‡")
        print(f"   2. å¢åŠ  HIDDEN_SIZES ç½‘ç»œå±‚æ•°æˆ–å®½åº¦")
        print(f"   3. è°ƒæ•´ DROPOUT_RATE æˆ– LEARNING_RATE")
        print(f"   4. å¢åŠ  EPOCHS è®­ç»ƒè½®æ•°")


if __name__ == "__main__":
    main()
